{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DSML Workshop 12` - Introduction to Neural Networks\n",
    "\n",
    "In this workshop we provide a very short introduction to neural networks in Python. This is very far from a comprehensive coverage of the topic but can provide a quick start for those who wish to learn more about the topic in their own time. We will cover a classification taks using `Keras` as our python package of choice. If you want to try and implement a NN from scratch there are several good online tutorials that can help you do so (see [here](https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6) for example)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biological inspiration\n",
    "The (for our purpose) smallest stand-alone element in the human brain is the neuron. Its understanding and computational recreation build the foundation for ANNs. A simplified image of a \"real\" neuron can be seen below.\n",
    "\n",
    "![](bio_neuron.png)\n",
    "\n",
    "Dendrites are connecting to the axons (or \"outputs\") of other neurons, for instance nerves in the sensory system or other processing neurons. In the nucleus, these input signals are aggregated and forwarded through the axon. The axon terminals then connect to further neurons to build the neural network. The connection between axon terminal and dendrite is what we are calling a synapse. In the human brain, there are billions of neurons and $10^{14} - 10^{15}$ synapses. If each synapse (or more precisely, its connection strength) would be represented by 8 bits or one byte, just storing these numbers would take 1000 TB already. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational implementation\n",
    "To re-create neural networks artificially, neurons have to be defined. The common mathematical model used for this purpose is depicted below.\n",
    "\n",
    "![](math_neuron.jpeg)\n",
    "\n",
    "From a certain number of input synapses $x_i$, signals come in with a weight factor of $w_i$. This represents the strength of the synapse. In the _nucleus_ these weighted inputs are aggregated and a bias is added (the bias is not shown in every model, but it does make the neural network more generalizable). After adding of the weighted inputs and the bias, everything is fed into a (non-linear) activation function. The output is then either fed forward to further neurons or is the output of your neural network. If there is only one neuron that takes direct inputs and whose output is your interest, the model is called a single-layer perceptron. Many of these neurons can create almost arbitrary logical connections and functions, making ANNs very powerful. In this case, we are talking about a multi-layer perceptron (MLP) model. \n",
    "\n",
    "![](mlp-network.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "The activation function is (to some degree) the heart of the neural network. Without a non-linear activation function, all hidden layers do not add any value, but are instead a complicated way to represent a linear model. Only with a non-linear activation function ANNs can re-create non-linear hypothesis functions. In the beginning of research on ANNs in the scope of AI, typically a unit-step was used as activation function. The unit step is $0$ for inputs smaller than $0$ and $1$ otherwise. The idea behing this is to recreate the behavior of a biological neuron that _fires_ if a certain threshold of inputs is exceeded. Today, other activation functions are more typically used. This is linked to better mathematical qualities in terms of learning behavior and convergence. Some of the most popular activation functions are:\n",
    "\n",
    "Sigmoid: $\\sigma(z) = \\frac{1}{1+exp(-z)}$\n",
    "\n",
    "Hyperbolic tangent: $\\sigma(z) = \\frac{2}{1+exp(-2z)} -1 $\n",
    "\n",
    "ReLU (Rectified Linear Unit): $\\sigma(z) = z\\quad  for\\ z>0,\\ 0\\ otherwise$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "As learning of ANNs is a non-trivial mathematical task, we are only aiming for an intuitive understanding here. Let's have a look at our complete MLP first.\n",
    "\n",
    "The general learning task consists of two steps, which are repeated until the algorithm converges:\n",
    "1. __Feedforward: Calculating the predicted output ŷ and the associated loss__. At first, we randomly assign values for the weights (and the biases). Based on the input features, the output value is calculated.\n",
    "2. __Backpropagation: Updating the weights W and biases b__. If the output value and the target value differ, the weights and biases are updated. To do this, it is calculated how much each weight and bias contributes to the error. Proportionally to this, they are then corrected (scaled with a small learning factor). In this sense, the updating rule has some similarity to gradient descent, only that is is propagated through the entire network, which is why this algorithm is called backpropagation.\n",
    "\n",
    "The training routine for a simple 2-layered MPL is shown in the below figure:\n",
    "\n",
    "![](training.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "The main hyperparameters of an MLP are: \n",
    "\n",
    "1. Number of hidden layers\n",
    "1. Number of nodes\n",
    "4. Activation function - could be understood as a hyperparameter, but that is typically not done\n",
    "\n",
    "The more layers and nodes there are (and the denser the network is, i.e. the more edges have a non-zero weight) the harder it gets to learn the model. That's the reason why bigger ANNs are normally not trained on a local computer anymore, but on specialized computers. Furthermore, there are additional libraries for python to improve the efficiency of ANNs, e.g. TensorFlow or Keras, which we take a first look at in today's tutorial.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Keras`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Keras` is one of the most popular Deep Learning libraries. `Tensorflow` and `Theano` are the most used numerical platforms in Python to build Deep Learning algorithms but they can be quite complex and difficult to use.\n",
    "\n",
    "Keras, by contrast, is easy to use and is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, Theano, and MXNet. The full documentation of the Keras API can be found [here](https://keras.io).\n",
    "\n",
    "Note that `scikit learn` also features an MLP implementation (see [here](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)). Yet, `Keras` has advanced to be one of the most popular frameworks used in practice, which is why we focus on it in this short tutorial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Keras` sits on top of `TensorFlow`. Therefore, we fist need to intall the latter library. To do so execute the following command:\n",
    "\n",
    "`conda install pip`\n",
    "\n",
    "`pip install tensorflow`\n",
    "\n",
    "When you are done use the following command via the command line to install `Keras`.\n",
    "\n",
    "`conda install -c conda-forge keras`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks for classification in `keras`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stay with our example, we will build a NN that predicts the class of a breast cancer sample by categorizing it as either malignant or benign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(suppress=True) # suppress scientific notation\n",
    "\n",
    "# supress versioning warnings of keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 18:10:33.035668: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import keras libraries\n",
    "# from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Preparation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>842302</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842517</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84300903</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84348301</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84358402</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean   \n",
       "id                                                                         \n",
       "842302           M        17.99         10.38          122.80     1001.0  \\\n",
       "842517           M        20.57         17.77          132.90     1326.0   \n",
       "84300903         M        19.69         21.25          130.00     1203.0   \n",
       "84348301         M        11.42         20.38           77.58      386.1   \n",
       "84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "          smoothness_mean  compactness_mean  concavity_mean   \n",
       "id                                                            \n",
       "842302            0.11840           0.27760          0.3001  \\\n",
       "842517            0.08474           0.07864          0.0869   \n",
       "84300903          0.10960           0.15990          0.1974   \n",
       "84348301          0.14250           0.28390          0.2414   \n",
       "84358402          0.10030           0.13280          0.1980   \n",
       "\n",
       "          concave points_mean  symmetry_mean  ...  texture_worst   \n",
       "id                                            ...                  \n",
       "842302                0.14710         0.2419  ...          17.33  \\\n",
       "842517                0.07017         0.1812  ...          23.41   \n",
       "84300903              0.12790         0.2069  ...          25.53   \n",
       "84348301              0.10520         0.2597  ...          26.50   \n",
       "84358402              0.10430         0.1809  ...          16.67   \n",
       "\n",
       "          perimeter_worst  area_worst  smoothness_worst  compactness_worst   \n",
       "id                                                                           \n",
       "842302             184.60      2019.0            0.1622             0.6656  \\\n",
       "842517             158.80      1956.0            0.1238             0.1866   \n",
       "84300903           152.50      1709.0            0.1444             0.4245   \n",
       "84348301            98.87       567.7            0.2098             0.8663   \n",
       "84358402           152.20      1575.0            0.1374             0.2050   \n",
       "\n",
       "          concavity_worst  concave points_worst  symmetry_worst   \n",
       "id                                                                \n",
       "842302             0.7119                0.2654          0.4601  \\\n",
       "842517             0.2416                0.1860          0.2750   \n",
       "84300903           0.4504                0.2430          0.3613   \n",
       "84348301           0.6869                0.2575          0.6638   \n",
       "84358402           0.4000                0.1625          0.2364   \n",
       "\n",
       "          fractal_dimension_worst  Unnamed: 32  \n",
       "id                                              \n",
       "842302                    0.11890          NaN  \n",
       "842517                    0.08902          NaN  \n",
       "84300903                  0.08758          NaN  \n",
       "84348301                  0.17300          NaN  \n",
       "84358402                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "cancer_df = pd.read_csv(\"breast_cancer.csv\", index_col = \"id\")\n",
    "cancer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x and Y\n",
    "X = cancer_df.iloc[:,1:31] # include full feature vector\n",
    "y = cancer_df[\"diagnosis\"]\n",
    "\n",
    "\n",
    "# encode categorical target verctor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>842302</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842517</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84300903</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84348301</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84358402</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926424</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926682</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926954</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927241</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92751</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          radius_mean  texture_mean  perimeter_mean  area_mean   \n",
       "id                                                               \n",
       "842302          17.99         10.38          122.80     1001.0  \\\n",
       "842517          20.57         17.77          132.90     1326.0   \n",
       "84300903        19.69         21.25          130.00     1203.0   \n",
       "84348301        11.42         20.38           77.58      386.1   \n",
       "84358402        20.29         14.34          135.10     1297.0   \n",
       "...               ...           ...             ...        ...   \n",
       "926424          21.56         22.39          142.00     1479.0   \n",
       "926682          20.13         28.25          131.20     1261.0   \n",
       "926954          16.60         28.08          108.30      858.1   \n",
       "927241          20.60         29.33          140.10     1265.0   \n",
       "92751            7.76         24.54           47.92      181.0   \n",
       "\n",
       "          smoothness_mean  compactness_mean  concavity_mean   \n",
       "id                                                            \n",
       "842302            0.11840           0.27760         0.30010  \\\n",
       "842517            0.08474           0.07864         0.08690   \n",
       "84300903          0.10960           0.15990         0.19740   \n",
       "84348301          0.14250           0.28390         0.24140   \n",
       "84358402          0.10030           0.13280         0.19800   \n",
       "...                   ...               ...             ...   \n",
       "926424            0.11100           0.11590         0.24390   \n",
       "926682            0.09780           0.10340         0.14400   \n",
       "926954            0.08455           0.10230         0.09251   \n",
       "927241            0.11780           0.27700         0.35140   \n",
       "92751             0.05263           0.04362         0.00000   \n",
       "\n",
       "          concave points_mean  symmetry_mean  fractal_dimension_mean  ...   \n",
       "id                                                                    ...   \n",
       "842302                0.14710         0.2419                 0.07871  ...  \\\n",
       "842517                0.07017         0.1812                 0.05667  ...   \n",
       "84300903              0.12790         0.2069                 0.05999  ...   \n",
       "84348301              0.10520         0.2597                 0.09744  ...   \n",
       "84358402              0.10430         0.1809                 0.05883  ...   \n",
       "...                       ...            ...                     ...  ...   \n",
       "926424                0.13890         0.1726                 0.05623  ...   \n",
       "926682                0.09791         0.1752                 0.05533  ...   \n",
       "926954                0.05302         0.1590                 0.05648  ...   \n",
       "927241                0.15200         0.2397                 0.07016  ...   \n",
       "92751                 0.00000         0.1587                 0.05884  ...   \n",
       "\n",
       "          radius_worst  texture_worst  perimeter_worst  area_worst   \n",
       "id                                                                   \n",
       "842302          25.380          17.33           184.60      2019.0  \\\n",
       "842517          24.990          23.41           158.80      1956.0   \n",
       "84300903        23.570          25.53           152.50      1709.0   \n",
       "84348301        14.910          26.50            98.87       567.7   \n",
       "84358402        22.540          16.67           152.20      1575.0   \n",
       "...                ...            ...              ...         ...   \n",
       "926424          25.450          26.40           166.10      2027.0   \n",
       "926682          23.690          38.25           155.00      1731.0   \n",
       "926954          18.980          34.12           126.70      1124.0   \n",
       "927241          25.740          39.42           184.60      1821.0   \n",
       "92751            9.456          30.37            59.16       268.6   \n",
       "\n",
       "          smoothness_worst  compactness_worst  concavity_worst   \n",
       "id                                                               \n",
       "842302             0.16220            0.66560           0.7119  \\\n",
       "842517             0.12380            0.18660           0.2416   \n",
       "84300903           0.14440            0.42450           0.4504   \n",
       "84348301           0.20980            0.86630           0.6869   \n",
       "84358402           0.13740            0.20500           0.4000   \n",
       "...                    ...                ...              ...   \n",
       "926424             0.14100            0.21130           0.4107   \n",
       "926682             0.11660            0.19220           0.3215   \n",
       "926954             0.11390            0.30940           0.3403   \n",
       "927241             0.16500            0.86810           0.9387   \n",
       "92751              0.08996            0.06444           0.0000   \n",
       "\n",
       "          concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "id                                                                       \n",
       "842302                  0.2654          0.4601                  0.11890  \n",
       "842517                  0.1860          0.2750                  0.08902  \n",
       "84300903                0.2430          0.3613                  0.08758  \n",
       "84348301                0.2575          0.6638                  0.17300  \n",
       "84358402                0.1625          0.2364                  0.07678  \n",
       "...                        ...             ...                      ...  \n",
       "926424                  0.2216          0.2060                  0.07115  \n",
       "926682                  0.1628          0.2572                  0.06637  \n",
       "926954                  0.1418          0.2218                  0.07820  \n",
       "927241                  0.2650          0.4087                  0.12400  \n",
       "92751                   0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine X\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine y\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Initializing and training the ANN__\n",
    "\n",
    "We start by defining the type of model we want to build. There are two types of models available in Keras: the [Sequential model](https://keras.io/models/sequential/) and the Model class used with [functional API](https://keras.io/models/model/). We select here the Sequential model and simply add the input-, 2 hidden- and the output-layers.\n",
    "\n",
    "Between them, we are using [dropout](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer) to prevent overfitting (dropout rate should be between 20% and 50% as a rule of thumb).\n",
    "\n",
    "At every layer, we use “Dense” which means that the nodes are fully connected (i.e., there are connection to each node in the next layer).\n",
    "\n",
    "The input-layer takes 30 inputs (because our feature vector includes 30 features) as input and outputs it with a shape of 15, which is the number of nodes in the first hidden layer that we define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pass the following parameters:\n",
    "\n",
    "- input_shape - number of columns of the dataset (only for input layer)\n",
    "\n",
    "- units - number of neurons and dimensionality of outputs to be fed to the next layer, if any\n",
    "\n",
    "- activation - activation function - we will use ReLU here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the input layer and the first hidden layer (with 30 nodes and 15 nodes respectively)\n",
    "classifier.add(Dense(input_shape = (30,), \n",
    "                     units=15,\n",
    "                     activation='relu'))\n",
    "\n",
    "# adding dropout to prevent overfitting\n",
    "classifier.add(Dropout(rate=0.1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add an additional second layer, also with 15 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the second hidden layer\n",
    "classifier.add(Dense(units= 15,\n",
    "                     activation='relu'))\n",
    "\n",
    "# adding dropout to prevent overfitting\n",
    "classifier.add(Dropout(rate=0.1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we add the output layer. Since we perform a binary classification, a single output node suffices. We use a sigmoidal activation function for this last node which is often used when dealing with binary classfication problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the output layer\n",
    "classifier.add(Dense(units = 1, \n",
    "                     activation='sigmoid'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compile the model to configure it for training. We add the following parameters:\n",
    "- `optimizer`: Here we use the adam optimizer, an optimizer with higher performance in many cases than stochastic gradient descent (SGD). See [here](https://keras.io/optimizers/) for a list of all optimizers implemented in `Keras`.\n",
    "- `loss`: specifies the loss to be minimized. In this example we use binary cross-entropy, a common loss for binary classification tasks. See [here](https://keras.io/losses/) for an overview of available losses in `Keras`. \n",
    "- `metrics`:  metric function is similar to a loss function, except that the results from evaluating a metric are not used when training the model and merely function as indicator of model performance to the data scientist. An overview of available metrics can be found [here](https://keras.io/metrics/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the ANN\n",
    "classifier.compile(optimizer=\"adam\", \n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 15)                465       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 15)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 15)                240       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 15)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 721\n",
      "Trainable params: 721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Can you sketch the network? And can you calculate the number of parameters per layer?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now able to train our model. We do this with a batch_size of 50 and for 50 epochs.\n",
    "\n",
    "- `batch_size` defines the number of samples that will be propagated through the network \n",
    "- `epoch` defines the number of iteration over the entire training data\n",
    "\n",
    "In general, a larger batch-size results in faster training, but does not always converge fast. A smaller batch-size is slower in training but it can converge faster. This is definitely problem-dependent and you need to try out a few different values (the standard batch-size is 32). The same goes for the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 3s 18ms/step - loss: 0.6354 - accuracy: 0.5503\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.5463 - accuracy: 0.7613\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4963 - accuracy: 0.8342\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4473 - accuracy: 0.8995\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3973 - accuracy: 0.9296\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3705 - accuracy: 0.9196\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3228 - accuracy: 0.9397\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2890 - accuracy: 0.9397\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2745 - accuracy: 0.9372\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2470 - accuracy: 0.9372\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2186 - accuracy: 0.9372\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1980 - accuracy: 0.9548\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.1880 - accuracy: 0.9548\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1656 - accuracy: 0.9523\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1464 - accuracy: 0.9648\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1430 - accuracy: 0.9698\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1453 - accuracy: 0.9623\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.1285 - accuracy: 0.9573\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1268 - accuracy: 0.9623\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1109 - accuracy: 0.9749\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1147 - accuracy: 0.9623\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1138 - accuracy: 0.9724\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0985 - accuracy: 0.9724\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1004 - accuracy: 0.9698\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1025 - accuracy: 0.9724\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0895 - accuracy: 0.9698\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0964 - accuracy: 0.9698\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0906 - accuracy: 0.9749\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0909 - accuracy: 0.9749\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0858 - accuracy: 0.9774\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0864 - accuracy: 0.9774\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0840 - accuracy: 0.9724\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0747 - accuracy: 0.9799\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0743 - accuracy: 0.9774\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0741 - accuracy: 0.9749\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0726 - accuracy: 0.9774\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0704 - accuracy: 0.9749\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0670 - accuracy: 0.9824\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0726 - accuracy: 0.9724\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0717 - accuracy: 0.9749\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0638 - accuracy: 0.9799\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0678 - accuracy: 0.9799\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0722 - accuracy: 0.9774\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0673 - accuracy: 0.9774\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0578 - accuracy: 0.9799\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0625 - accuracy: 0.9799\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0724 - accuracy: 0.9749\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0606 - accuracy: 0.9799\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0531 - accuracy: 0.9849\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0587 - accuracy: 0.9849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13b6f5dd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the ANN to the training set\n",
    "classifier.fit(X_train, y_train, batch_size=50, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 2ms/step\n",
      "6/6 [==============================] - 0s 6ms/step\n",
      "6/6 [==============================] - 0s 15ms/step\n",
      "Confusion Matrix\n",
      "[[108   0]\n",
      " [  2  61]]\n",
      "\n",
      "Accuracy\n",
      "0.9883\n",
      "\n",
      "Precision\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# report classification performance on test set\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, classifier.predict(X_test).round(decimals=0).astype(int))\n",
    "accuracy_score = accuracy_score(y_test, classifier.predict(X_test).round(decimals=0).astype(int))\n",
    "precision_score = precision_score(y_test, classifier.predict(X_test).round(decimals=0).astype(int))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix)\n",
    "print()\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score.round(decimals=4))\n",
    "print()\n",
    "print(\"Precision\")\n",
    "print(precision_score.round(decimals=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this predictive performance is higher than anything we have achieved with traditional models in previous workshops thus far!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks for regression in `Keras`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can also be trained for regression tasks. The logic is exactly the same, yet some of the parameters, such as loss, metrics, input and ouput as well as typical activation functions might have to be adapted to the specific case. We will not cover ANN regression in this tutorial, which is simply meant as an introduction to the topic. There are a range of very good tutorials online, which we encourage you to take a look at (for example [here](https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/))."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Starting with a very simple architecture and trying out different values for the key hyperparameters (e.g. via grid search), design, train, validate and test an ANN for the electricity demand dataset which we have worked with throughout this course. What predictive performance do you achieve on the test set? How do you avoid overfitting?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are performance metrics of some of our previous models (using only high temperature as raw input to predict peak demand):\n",
    "\n",
    "**KNN regression**\n",
    "- MAE: 0.128 GW\n",
    "- RMSE: 0.159 GW\n",
    "- R2: 0.755\n",
    "\n",
    "**Tree-based regression**\n",
    "- MAE: 0.134 GW\n",
    "- RMSE: 0.169 GW\n",
    "- R2: 0.725\n",
    "\n",
    "Can you design a Neural network that performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AVG</th>\n",
       "      <th>MAX</th>\n",
       "      <th>MIN</th>\n",
       "      <th>Total</th>\n",
       "      <th>High_temp</th>\n",
       "      <th>Avg_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01.01.2013</td>\n",
       "      <td>1.598524</td>\n",
       "      <td>1.859947</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>38.368031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02.01.2013</td>\n",
       "      <td>1.809347</td>\n",
       "      <td>2.054215</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>43.428194</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>-6.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03.01.2013</td>\n",
       "      <td>1.832822</td>\n",
       "      <td>2.049550</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>43.991607</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-6.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04.01.2013</td>\n",
       "      <td>1.812699</td>\n",
       "      <td>2.008168</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>43.508609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05.01.2013</td>\n",
       "      <td>1.662036</td>\n",
       "      <td>1.838251</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>39.892360</td>\n",
       "      <td>1.7</td>\n",
       "      <td>-1.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       AVG       MAX       MIN      Total  High_temp  Avg_temp\n",
       "0  01.01.2013  1.598524  1.859947  0.001599  38.368031        0.0     -1.68\n",
       "1  02.01.2013  1.809347  2.054215  0.001809  43.428194       -3.9     -6.58\n",
       "2  03.01.2013  1.832822  2.049550  0.001833  43.991607        0.6     -6.12\n",
       "3  04.01.2013  1.812699  2.008168  0.001813  43.508609        0.0     -1.95\n",
       "4  05.01.2013  1.662036  1.838251  0.001662  39.892360        1.7     -1.47"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "pittsburgh_df = pd.read_csv(\"Pittsburgh_load_data.csv\")\n",
    "\n",
    "pittsburgh_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x and y\n",
    "X = pittsburgh_df[\"High_temp\"].values.reshape(-1,1)\n",
    "y = pittsburgh_df[\"MAX\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_regression(hidden_layers = 2, activation_hidden=\"relu\", units_hidden=15, dropout=0.1, batch_size=50, epochs=50):\n",
    "    # initialize the ANN\n",
    "    regression = Sequential()\n",
    "\n",
    "    # adding the input layer and the first hidden layer (with 1 node and 15 nodes respectively)\n",
    "    regression.add(Dense(input_shape = (1,), \n",
    "                        units=units_hidden,\n",
    "                        activation=activation_hidden))\n",
    "\n",
    "    # adding dropout to prevent overfitting\n",
    "    regression.add(Dropout(rate=dropout))\n",
    "\n",
    "    for i in range(hidden_layers-1):\n",
    "        # adding hidden layer\n",
    "        regression.add(Dense(units=units_hidden,\n",
    "                            activation=activation_hidden))\n",
    "\n",
    "        # adding dropout to prevent overfitting\n",
    "        regression.add(Dropout(rate=dropout))\n",
    "\n",
    "    # adding the output layer\n",
    "    regression.add(Dense(units = 1)) # unlike classification, no activation function necessary in regression case\n",
    "\n",
    "    # compiling the ANN\n",
    "    regression.compile(optimizer=\"adam\", \n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[\"mean_squared_error\"])\n",
    "    \n",
    "    print(regression.summary())\n",
    "\n",
    "    # fitting the ANN to the training set\n",
    "    regression.fit(X_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "    # compute test set performance measures\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "    print(\"Test set performance:\")\n",
    "\n",
    "    print(\"MAE:\",mean_absolute_error(y_test, regression.predict(X_test)), \"GW\")\n",
    "    print(\"RMSE:\",(mean_squared_error(y_test, regression.predict(X_test)))**(0.5), \"GW\")  \n",
    "    print(\"R2:\",r2_score(y_test, regression.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 15)                30        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 15)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 15)                240       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 15)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 286\n",
      "Trainable params: 286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "26/26 [==============================] - 3s 6ms/step - loss: 3.0332 - mean_squared_error: 3.0332\n",
      "Epoch 2/50\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 2.2771 - mean_squared_error: 2.2771\n",
      "Epoch 3/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.4552 - mean_squared_error: 1.4552\n",
      "Epoch 4/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.7232 - mean_squared_error: 0.7232\n",
      "Epoch 5/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.3134 - mean_squared_error: 0.3134\n",
      "Epoch 6/50\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1754 - mean_squared_error: 0.1754\n",
      "Epoch 7/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.1558 - mean_squared_error: 0.1558\n",
      "Epoch 8/50\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1476 - mean_squared_error: 0.1476\n",
      "Epoch 9/50\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1321 - mean_squared_error: 0.1321\n",
      "Epoch 10/50\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1351 - mean_squared_error: 0.1351\n",
      "Epoch 11/50\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1300 - mean_squared_error: 0.1300\n",
      "Epoch 12/50\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1185 - mean_squared_error: 0.1185\n",
      "Epoch 13/50\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1213 - mean_squared_error: 0.1213\n",
      "Epoch 14/50\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1160 - mean_squared_error: 0.1160\n",
      "Epoch 15/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.1153 - mean_squared_error: 0.1153\n",
      "Epoch 16/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.1124 - mean_squared_error: 0.1124\n",
      "Epoch 17/50\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1147 - mean_squared_error: 0.1147\n",
      "Epoch 18/50\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1078 - mean_squared_error: 0.1078\n",
      "Epoch 19/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.1030 - mean_squared_error: 0.1030\n",
      "Epoch 20/50\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1118 - mean_squared_error: 0.1118\n",
      "Epoch 21/50\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1020 - mean_squared_error: 0.1020\n",
      "Epoch 22/50\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1081 - mean_squared_error: 0.1081\n",
      "Epoch 23/50\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1057 - mean_squared_error: 0.1057\n",
      "Epoch 24/50\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0980 - mean_squared_error: 0.0980\n",
      "Epoch 25/50\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 0.1071 - mean_squared_error: 0.1071\n",
      "Epoch 26/50\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1015 - mean_squared_error: 0.1015\n",
      "Epoch 27/50\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0966 - mean_squared_error: 0.0966\n",
      "Epoch 28/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.1006 - mean_squared_error: 0.1006\n",
      "Epoch 29/50\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.0937 - mean_squared_error: 0.0937\n",
      "Epoch 30/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0929 - mean_squared_error: 0.0929\n",
      "Epoch 31/50\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0957 - mean_squared_error: 0.0957\n",
      "Epoch 32/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0862 - mean_squared_error: 0.0862\n",
      "Epoch 33/50\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0923 - mean_squared_error: 0.0923\n",
      "Epoch 34/50\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0942 - mean_squared_error: 0.0942\n",
      "Epoch 35/50\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.0878 - mean_squared_error: 0.0878\n",
      "Epoch 36/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0823 - mean_squared_error: 0.0823\n",
      "Epoch 37/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0884 - mean_squared_error: 0.0884\n",
      "Epoch 38/50\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.0807 - mean_squared_error: 0.0807\n",
      "Epoch 39/50\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0803 - mean_squared_error: 0.0803\n",
      "Epoch 40/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0866 - mean_squared_error: 0.0866\n",
      "Epoch 41/50\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0841 - mean_squared_error: 0.0841\n",
      "Epoch 42/50\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0832 - mean_squared_error: 0.0832\n",
      "Epoch 43/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0775 - mean_squared_error: 0.0775\n",
      "Epoch 44/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0730 - mean_squared_error: 0.0730\n",
      "Epoch 45/50\n",
      "26/26 [==============================] - 0s 19ms/step - loss: 0.0761 - mean_squared_error: 0.0761\n",
      "Epoch 46/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0859 - mean_squared_error: 0.0859\n",
      "Epoch 47/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0782 - mean_squared_error: 0.0782\n",
      "Epoch 48/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0719 - mean_squared_error: 0.0719\n",
      "Epoch 49/50\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0764 - mean_squared_error: 0.0764\n",
      "Epoch 50/50\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.0754 - mean_squared_error: 0.0754\n",
      "Test set performance:\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "MAE: 0.14694350890834837 GW\n",
      "18/18 [==============================] - 0s 6ms/step\n",
      "RMSE: 0.17970535694791562 GW\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "R2: 0.7014465774330503\n"
     ]
    }
   ],
   "source": [
    "nn_regression()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "a0e5296c3657c6b7a86a9bab3436e28ffbdd8356439efa15fab08846068601a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
