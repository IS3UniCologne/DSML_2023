{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DSML_WS_03` - Introduction to Pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will continue with our introduction series to key python data science libraries. Today we will focus on `Pandas`.\n",
    "\n",
    "We will go through the following:\n",
    "\n",
    "- **Introduction to `Pandas` objects**: Learn what `Pandas` Series, DataFrames and Indices are and what they are used for\n",
    "- **Data selection in `Pandas`**: Idex rows, columns and individual items in a `Pandas` Series and DataFrame\n",
    "- **Data analysis and manipulation in `Pandas`**: Perform advanced hierarchical indexing on your dataset, run simple descriptives on your dataset, basic handling of missing data, carry out aggregation and grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## `Pandas`\n",
    "\n",
    "Pandas is a newer package built on top of `NumPy`, and provides an efficient implementation of a `DataFrame`, the core Pandas object. DataFrames are essentially multidimensional arrays with attached row and column labels, and often with heterogeneous types and/or missing data. As well as offering a convenient storage interface for labeled data, Pandas implements a number of powerful data operations familiar to users of both database frameworks and spreadsheet programs. We will use `Pandas` as the main tool to structure, manipulate and store data throughout this course. As such Pandas constitutes a core data science library that all of you should be very well familiar with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to `Pandas` objects\n",
    "At the very basic level, Pandas objects can be thought of as enhanced versions of NumPy structured arrays in which the rows and columns are identified with labels rather than simple integer indices. Pandas provides a host of useful tools, methods, and functionality on top of the basic data structures, but nearly everything that follows will require an understanding of what these structures are. Thus, before we go any further, let's introduce these three fundamental Pandas data structures: the Series, DataFrame, and Index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `Pandas Series` Object\n",
    "A Pandas Series is a one-dimensional array of indexed data. It can be created from a list or array as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Series using pd.Series and assign it to variable A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the type of the created Series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in the output, the Series wraps both a sequence of values and a sequence of indices, which we can access with the values and index attributes. The values are simply a familiar NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the values of A using A.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index is an array-like object of type pd.Index, which we'll discuss in more detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the indices of A using A.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what we've seen so far, it may look like the Series object is basically interchangeable with a one-dimensional NumPy array. **The essential difference is the presence of the index**: while the Numpy Array has an implicitly defined integer index used to access the values, the Pandas Series has an explicitly defined index associated with the values.\n",
    "This explicit index definition gives the Series object additional capabilities. For example, the index need not be an integer, but can consist of values of any desired type. For example, if we wish, we can use strings as an index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a Series B and assign strings as index using \"index=[]\"\n",
    "B = pd.Series([0.25, 0.5, 0.75, 1.0],index=['a', 'b', 'c', 'd'])\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, you can think of a Pandas Series a bit like a specialization of a Python dictionary. A dictionary is a structure that maps arbitrary keys to a set of arbitrary values, and a Series is a structure which maps typed keys to a set of typed values. This typing is important: just as the type-specific compiled code behind a NumPy array makes it more efficient than a Python list for certain operations, the type information of a Pandas Series makes it much more efficient than Python dictionaries for certain operations.\n",
    "The Series-as-dictionary analogy can be made even more clear by constructing a Series object directly from a Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the following dictionary of state populations\n",
    "population_dict = {'California': 38332521,\n",
    "                   'Texas': 2644819,\n",
    "                   'New York': 19651127,\n",
    "                   'Florida': 19552860,\n",
    "                   'Illinois': 12882135}\n",
    "\n",
    "population_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Series from population_dict\n",
    "population_series = pd.Series(population_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike a dictionary, though, the Series also supports array-style operations such as slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning a single element works both with dictionaries and Series\n",
    "print(population_dict[\"California\"])\n",
    "print(population_series[\"California\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to return the first three populations (California, Texas, New York) from population_dict\n",
    "population_dict[\"California\":\"New York\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to return the first three populations (California, Texas, New York) from population_series\n",
    "population_series[\"California\":\"New York\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `Pandas DataFrame` Object\n",
    "The next fundamental structure in `Pandas`  is the `DataFrame` . Like the Series object discussed in the previous section, the DataFrame can be thought of either as a generalization of a NumPy array, or as a specialization of a Python dictionary. We'll now take a look at each of these perspectives.\n",
    "\n",
    "If a Series is analogous to a one-dimensional array with flexible indices, a DataFrame is analogous to a two-dimensional array with both flexible row indices and flexible column names. Just as you might think of a two-dimensional array as an ordered sequence of aligned one-dimensional columns, you can think of a DataFrame as a sequence of aligned Series objects. Here, by \"aligned\" we mean that they share the same index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a second series of state areas\n",
    "area_dict = {'California': 423967, 'Texas': 695662, 'New York': 141297,\n",
    "             'Florida': 170312, 'Illinois': 149995}\n",
    "area_series = pd.Series(area_dict)\n",
    "area_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have this along with the population Series from before, we can use a dictionary to construct a single two-dimensional object containing this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Pandas Dataframe called states using pd.DataFrame\n",
    "states = pd.DataFrame({'population': population_series,\n",
    "                       'area': area_series})\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the type of states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the values of states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the Series object, the DataFrame has an index attribute that gives access to the index labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the indices of states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the DataFrame has a columns attribute, which is an Index object holding the column labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the column names of states using .columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Thus the DataFrame can be thought of as a generalization of a two-dimensional NumPy array, where both the rows and columns have a generalized index for accessing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Pandas Index` Object\n",
    "We have seen here that both the Series and DataFrame objects contain an explicit index that lets you reference and modify data. This Index object is an interesting structure in itself, and it can be thought of either as an immutable array or as an ordered set (technically a multi-set, as Index objects may contain repeated values). Those views have some interesting consequences in the operations available on Index objects. As a simple example, let's construct an Index from a list of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an Index object ind using pd.Index\n",
    "ind = pd.Index([2, 3, 5, 7, 11])\n",
    "ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Index in many ways operates like an array. For example, we can use standard Python indexing notation to retrieve values or slices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the first element of ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return every second object of ind\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index objects also have many of the attributes familiar from NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the size, shape, number of dimensions and data type of ind\n",
    "print(\n",
    "    \"Size:\", ind.size,\n",
    "    \"\\nShape:\", ind.shape, \n",
    "    \"\\nNumber of dimensions:\", ind.ndim, \n",
    "    \"\\nDatatype:\", ind.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One difference between Index objects and NumPy arrays is that indices are immutableâ€“that is, they cannot be modified via the normal means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to change the value of the first element of ind\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting values in `Pandas`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terminology used below (e.g. extracting, selecting, slicing...) can be confusing sometimes. For a quick overview, refer to https://avichawla.substack.com/p/are-you-sure-you-are-using-the-correct for a nice, intuitive overview."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting data from a Series\n",
    "\n",
    "As we saw in the previous section, a Series object acts in many ways like a one-dimensional NumPy array, and in many ways like a standard Python dictionary. If we keep these two overlapping analogies in mind, it will help us to understand the patterns of data indexing and extraction in these arrays.\n",
    "\n",
    "Like a dictionary, the Series object provides a mapping from a collection of keys (i.e. the index) to a collection of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember our Series B\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the first element of B using an explicit index (as we are used to from Python dictionaries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use dictionary-like Python expressions and methods to examine the keys/indices and values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the .keys() method to extract the indices of B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the .items() method to extract keys,value pairs\n",
    "list(B.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series objects can even be modified with a dictionary-like syntax. Just as you can extend a dictionary by assigning to a new key, you can extend a Series by assigning to a new index value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new key,value pair to B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the value of the first element of B\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Series builds on this dictionary-like interface and provides array-style item extraction via the same basic mechanisms as NumPy arrays. Examples of these are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the first element of B using an implicit index (as we are used to from NumPy arrays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing by explicit index: extract the first three elements of B\n",
    "B['?':'?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing by implicit index: extract the first three elements of B\n",
    "B['?':'?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple explicit indexing: extract multiple elements using a list of indices\n",
    "B[['?', '?', '?']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when slicing with an explicit index (i.e., data['a':'c']), the final index is included in the slice, while when slicing with an implicit index (i.e., data[0:2]), the final index is excluded from the slice.\n",
    "\n",
    "**Indexers: loc and iloc**\n",
    "\n",
    "These slicing and indexing conventions can be a source of confusion. For example, if your Series has an explicit integer index, an indexing operation such as B[1] will use the explicit indices, while a slicing operation like B[1:3] will use the implicit Python-style index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a new Series with explicit integer indices\n",
    "C = pd.Series(['a', 'b', 'c'], index=[1, 3, 5])\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicit index when extracting a single row\n",
    "C[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implicit index when slicing multiple rows\n",
    "C[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this potential confusion in the case of integer indexes, Pandas provides some special indexer attributes that explicitly expose certain indexing schemes. These are not functional methods, but attributes that expose a particular slicing interface to the data in the Series.\n",
    "First, the `.loc` attribute allows indexing and slicing that always references the **explicit index**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the value \"a\" using .loc[]\n",
    "C.loc['?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the values \"a\" and \"b\" using .loc[]\n",
    "C.loc['?':'?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.iloc` attribute allows indexing and slicing that always references the **implicit Python-style index**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the value \"a\" using .iloc[]\n",
    "C.iloc['?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the values \"a\" and \"b\" using .iloc[]\n",
    "C.iloc['?':'?']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One guiding principle of Python code is that **\"explicit is better than implicit\"**. The explicit nature of `.loc` and `.iloc` make them very useful in maintaining clean and readable code; especially in the case of integer indices, I recommend using these both to make code easier to read and understand, and to prevent subtle bugs due to the mixed indexing/slicing convention."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting data from a  DataFrame\n",
    "\n",
    "Recall that a DataFrame acts in many ways like a two-dimensional or structured array, and in other ways like a dictionary of Series structures sharing the same index. These analogies can be helpful to keep in mind as we explore data extraction within this structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember our states DataFrame\n",
    "states"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual Series that make up the columns of the DataFrame can be selected via dictionary-style indexing of the column name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the area column of states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, you can also select a column by appending its name to the name of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the values of the area column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with the Series objects discussed earlier, this dictionary-style syntax can also be used to modify the object, in this case adding a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add the population density (\"pop_density\") to states, which is population divided by area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the pop_density column from states\n",
    "states[\"pop_density\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, we can also view the DataFrame as an enhanced two-dimensional array. We can examine the raw underlying data array using the values attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the data of states using .values\n",
    "states.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this picture in mind, many familiar array-like observations can be done on the DataFrame itself. For example, we can transpose the full DataFrame to swap rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose states using .T\n",
    "states.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to indexing of DataFrame objects, however, it is clear that the dictionary-style indexing of columns precludes our ability to simply treat it as a NumPy array. In particular, passing a single index to an array accesses a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a row\n",
    "states.values[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and passing a single \"index\" to a DataFrame accesses a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a column\n",
    "states[\"area\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between NumPy arrays and DataFrames becomes evident also when attempting to access a single element. While you might remember from last week's workshop that we can access an element in a two-dimensional array as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the element in the first row and second column\n",
    "states.values[0,1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...the same does not work for our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results in an error message\n",
    "states[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus for array-style indexing, we need another convention. Here Pandas again uses the `.loc` and `.iloc` indexers mentioned earlier. Using the `.iloc` indexer, we can index the underlying array as if it is a simple NumPy array (using the implicit Python-style index), but the DataFrame index and column labels are maintained in the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the first three rows and first two columns of states using iloc (implicit indices)\n",
    "states.iloc['?','?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, using the `.loc` indexer we can index the underlying data in an array-like style but using the explicit index and column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the first three rows and first two columns of states using loc (explicit indices)\n",
    "states.loc['?', '?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pass conditions to the indexer, a technique called **masking**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking with .loc: return the states with pop_density > 100\n",
    "states.loc[states.pop_density > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can define multiple conditions as follows\n",
    "(states[\"area\"] > 150000) & (states[\"pop_density\"] > 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# masking with explicit column index\n",
    "states[(states[\"area\"] > 150000) & (states[\"pop_density\"] > 50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise**: Produce a DF of states with population >15M and <30M. Name this DF `mid_sized_states_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_sized_states_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any of these indexing conventions may also be used to set or modify values; this is done in the standard way that you might be accustomed to from working with NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change the population density of California to 100\n",
    "states.loc[\"California\",\"pop_density\"] = 100\n",
    "states"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Excercise**: Set the population of Florida to 19M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the following line of code, where we try to add a column to our mid_sized_states_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_sized_states_df[\"area_x10\"] = mid_sized_states_df[\"area\"]*10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above error is something you will encounter when working with pandas Dataframes. It is intended to give you a warning that the behavior that you will get executing this code is not always what you expect. It comes from the definition of mid_sized_states_df.\n",
    "\n",
    "Pandas is unsure whether you wanted to add values to the 'states' dataframe or the 'mid_sized_states_df' dataframe!\n",
    "To get rid of this error, you can explicitly copy the dataframe so pandas knows we are trying to add a column to the 'new' dataframe 'mid_sized_states_df'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_sized_states_df = states[(states[\"population\"] > 15000000) & (states[\"population\"] < 30000000)].copy()\n",
    "mid_sized_states_df[\"area_x10\"] = mid_sized_states_df[\"area\"]*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_sized_states_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis and manipulation in `Pandas`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Indexing\n",
    "\n",
    "**Using MultiIndex to create hierarchichal DataFrames**\n",
    "\n",
    "Often it is useful to store higher-dimensional data, i.e. data indexed by more than one or two keys. The most appropriate Pandas function for this is `MultiIndex`. We will demonstrate using a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a 2-D example, first specifying the outer and inner dimension as well as the values\n",
    "Outer = ['California','California',\n",
    "         'New York','New York',\n",
    "         'Texas','Texas']\n",
    "\n",
    "Inner = [2000,2010,\n",
    "         2000,2010,\n",
    "         2000,2010]\n",
    "\n",
    "populations = [33871648, 37253956,\n",
    "               18976457, 19378102,\n",
    "               20851820, 25145561]\n",
    "\n",
    "# From the outer and inner dimensions we create an index\n",
    "\n",
    "index_list = list(zip(Outer,Inner))\n",
    "index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from index_list we can create a multi index\n",
    "index_object = pd.MultiIndex.from_tuples(index_list)\n",
    "index_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we create a data frame with hierarchichal indices\n",
    "df = pd.DataFrame(index=index_object,\n",
    "                  data=populations,\n",
    "                  columns=[\"Population\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add more columns to df\n",
    "area = [164,164, #California \n",
    "        55,55,   #NY State\n",
    "        269,269] #Texas\n",
    "\n",
    "df[\"Area\"] = area\n",
    "df[\"Density\"] = df[\"Population\"]/df[\"Area\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, we can easily transpose this DataFrame\n",
    "df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MultiIndex as extra dimension**\n",
    "\n",
    "You might notice that we could have stored the same data using a simple DataFrame with index and column labels. The `unstack()` method let's us convert a hierachically indexed Series into a conventionally indexed DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Population'].unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick data analysis in `Pandas`\n",
    "\n",
    "In practice your dataset will often be considerably larger than the illustrative example above. In these cases it is often useful to run brief descriptive statistical analyses on the set, which will help to get a first feel for the data. \n",
    "\n",
    "We will demonstrate how to do this using the provided `iris.csv` dataset. This is a famous dataset used frequently for educational purposes in data science. You can read up on the dataset, its content and its origins here: https://en.wikipedia.org/wiki/Iris_flower_data_set.\n",
    "\n",
    "The iris dataset contains measurements for 150 iris flowers from three different species.\n",
    "\n",
    "The __three classes__ in the Iris dataset:\n",
    "* Iris-setosa (n=50)\n",
    "* Iris-versicolor (n=50)\n",
    "* Iris-virginica (n=50)\n",
    "\n",
    "The __four features__ of the Iris dataset:\n",
    "* sepal length in cm\n",
    "* sepal width in cm\n",
    "* petal length in cm\n",
    "* petal width in cm\n",
    "\n",
    "![Iris dataset explained (Source: apsl.net)](iris.original.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first read in the dataset as a DataFrame\n",
    "# for the indirect reference \"iris.csv\" to work make sure it is contained in the same folder as your notebook\n",
    "# You can set the index by using the index_col function\n",
    "iris_set = pd.read_csv(\"iris.csv\", sep=\",\", index_col=\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the first 5 rows using .head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return all column names using .columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the .describe()-function provides an overview of key descriptive statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the .info()-function provides an overview of null values and datatypes per column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also focus your analysis on individual features by indexing them\n",
    "iris_set[\"Sepal.Width\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additionally you can call the functions seperately, e.g. .count()\n",
    "iris_set.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling missing numerical data (`NaN` values)\n",
    "If you have paid attention to the descriptive statistics above you will have noticed that the __count__ of values differs across features. This is a first indication for missing numerical data. In the real world you will often encounter incomplete and noisy data which will require pre-processing before you can apply data science and machine learning tools to them. In this section we will provide a quick overview on how to deal with missing data in your data sets. \n",
    "Note that Python has two ways of highlighting missing data:\n",
    "* `NaN` - (acronym for Not a Number), is a special floating-point value recognized by all systems that use the standard IEEE floating-point representation\n",
    "* `None` - Python-specific object that is often used for missing data in Python code. Because it is a Python object, `None` can only be used in arrays with data type 'object' (i.e., arrays of Python objects) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detecting missing numerical data**\n",
    "\n",
    "Pandas data structures have two useful methods for detecting null data: `isnull()` and `notnull()`. Either one will return a Boolean mask over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# .isnull() returns \"True\" for every missing numerical value in the dataset\n",
    "iris_set.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .notnull() returns \"False\" for every missing numerical value in the dataset\n",
    "iris_set.notnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Dealing with missing numerical data\n",
    "In essence two approaches to dealing with missing data exist:\n",
    "* __Elimination__: Dropping null values from the dataset\n",
    "* __Imputation__: Imputing/replacing null values with numerical values/estimates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elimination (i.e. dropping null values)**\n",
    "\n",
    "We cannot drop single values from a data frame; we can only drop full rows or full columns. Depending on the application, you might want one or the other, so `dropna()` gives a number of options:\n",
    "\n",
    "We use the `dropna(axis=0)` function to drop all rows from the dataset that include null values. Note that `dropna()` will not modify your dataframe unless you call `dropna(axis=0,inplace=True)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's drop all rows that include null values but not modify the dataframe\n",
    "iris_set.dropna(axis=0, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `dropna(axis=1)` function to drop all columns from the dataset that incl. null values. Note that, also in this case, `dropna()` will not modify your dataframe unless you call `dropna(axis=1,inplace=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's drop all columns that include null values but not modify the dataframe\n",
    "iris_set.dropna(axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that our dataframe has not been modified, i.e. we still have all 150 rows and 5 columns\n",
    "iris_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can therefore also use dropna() to easily identify the number of rows with missing values \n",
    "len(iris_set)-len(iris_set.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you whish to clean the data with dropna it is good practice to define a new data frame\n",
    "iris_set_clean = iris_set.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the number of rows of our original and clean dataframes\n",
    "print(\"iris_set:\",len(iris_set))\n",
    "print(\"iris_set_clean:\",len(iris_set_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise**: Use the technique of **masking** to display all records with NaN values for Sepal.Length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imputation (i.e. filling null values)**\n",
    "\n",
    "Imputation fills null values with numerical values chosen by the data scientist. For this, `fillna()`provides the appropriate tools.\n",
    "\n",
    "The data scientist will usually choose one of the following methods:\n",
    "* Fill with zeros: `fillna(0)`\n",
    "* Conduct a forward fill (i.e. filling NaN values with data from previous observation): `fillna(method=\"ffill\")`\n",
    "* Conduct a backward fill (i.e. filling NaN values with data from following observation): `fillna(method=\"bfill\")`\n",
    "* Fill with the column mean,max,min, etc.: `df[\"Col_name\"].fillna(value=df[\"Col_name\"].mean())`\n",
    "* Custom fill depending on the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's replace the NaN values in Sepal.Length with the average sepal length; modify the dataframe by setting \"inplace = True\"\n",
    "iris_set[\"Sepal.Length\"].fillna(value=iris_set[\"Sepal.Length\"].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the new values of the 123rd and 124th flower\n",
    "print(iris_set.loc[123,\"Sepal.Length\"])\n",
    "print(iris_set.loc[124,\"Sepal.Length\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data aggregation and grouping\n",
    "The `groupby()` method allows for grouping of rows and the application of aggregation functions to these grouped rows. It is a highly useful method in data science and used extensively. We will return to the iris dataset for illustration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get some summary statistics for the different species (setosa, versicolor, virginica), e.g. the mean of our four features\n",
    "species_groups_mean = iris_set.groupby(\"Species\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_groups_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also use other aggregation routines, including .sum(), .max(), .min(), .count()\n",
    "species_groups_min = iris_set.groupby(\"Species\").min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_groups_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out some more aggregation routines on the sample here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also apply multiple aggregation routines at the same time using .agg()\n",
    "species_groups_agg = iris_set.groupby(\"Species\").agg(['mean', 'max', 'count'])\n",
    "species_groups_agg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you can also profide custom aggregation routines, which is useful if features follow different aggregation routines. As an illustrative (albeit not very realistic example) we will implement the following aggregation rules:\n",
    "\n",
    "- Sepal.Length: `max()`\n",
    "- Sepal.Width: `mean()`\n",
    "- Petal.Length: `min()`\n",
    "- Petal.Width: `mean()`\n",
    "\n",
    "Custom aggregation is, again, implemented using the `.agg()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, define a dictionary that maps features to aggreagation routines\n",
    "agg_dict = {'Sepal.Length':\"max\", 'Sepal.Width':\"mean\", 'Petal.Length':\"min\", 'Petal.Width':\"mean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second, pass the dictionary as an argument to .agg()\n",
    "species_groups_custom = iris_set.groupby(\"Species\").agg(agg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_groups_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brings us to the end of `Workshop_03`. Next week we will cover visualization techniques using `Matplotlib`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
